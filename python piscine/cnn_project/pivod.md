detectionモデルに関してのこれからの進捗
・originで、image_with_classが100枚以下のクラスについては、学習に使用しない方針がよいかもしれない
・一枚の画像に複数のクラスがアノテーションされているデータに関しては、少ないデータ数のクラスがアノテーションされているデータの保護を優先したほうがいい
->logicとしては少ないクラスのアノテーション画像をrare_classとして振り分ける方針でいくことも考えられる
（データ数に関しては+-100枚ぐらいなら許容だと考えられる）

crackとpoor drainageが同時に写るものがあったとして,crackの数が500を超えたから使わないようにする。
少ないクラスが写る画像はなるべく拾うようにするのがいいかもしれない
画像のなかでアノテーションされた領域の広さ（ピクセル数）に着目してみるのもありかもしれない

IOUをyolov8のモデルで可視化をすることができるように考える必要がある

ステップ0:
・画像とGT txtは1対1対応（stem名一致）
・GT txtはYOLO形式（class cx cy w h, 正規化）
・class_idの対応が data.yaml のnamesと一致

ステップ１：
・小物体が多いとか、極端にでかいboxばかりとか見える
ｰ>bboxなら面積比が現実的である

これって一つの画像単位で見るのがいいのではないか
一枚の画像に対してどのくらいのクラスがどのくらいの全体の画像に対して占めているかを確認することが重要だと感じる

予測jsonをtxtに変換して比較
Ultralyticsの予測JSON(COCO形式)を
　・画像ごとに予測box一覧(class, conf, xyxy)へ整理
　・GT txtと同じ画像に対応ずけ

Percision/Recallはなにを比較しているのか
各画像にある
・GT（正解）：アノテーションされたbbox(class付き)
・Pred(予測)：モデルが出したbbox(class+conf付き)
この2つをIoUマッチングして、
・TP(当たり)：同じクラスで、IoUが閾値以上(例：0.5)で1対1対応ができた予測
・FP（外れ）：どのGTにも対応できなかった予測（誤検出、クラス違い、位置ズレでIoU不足もここ）
・FN（見逃し）：対応する予測が見つからなかったGT（未検出）

Percison/Recall
・Percision=TP/(TP + FP)
→ 「出した予測のうち、正解だった割合」ｰ>予測を出しすぎて誤検出が多いと下がる。
・Recall=TP/(TP+FN)
->「正解をどれだけ拾えたか」ｰ>見逃し(FN)が多いと下がる

比較しているもの
ニュアンスとしては、同じ画像に対するGTboxとPredboxを比較している
画像のピクセルを直接比較しているのは、セグメのDice/IoU(マスク)とかの世界で、検出(bbox)では基本box同士の比較

